(16:13) the course of their readings um the next the next question is um uh uh from from one of our folks at Google so I've been wondering um since large language models learn from massive sources of data do large language models simply interpolate within their training data or can they go beyond to make new discoveries um and Aman how about how about you answer that I I think uh and then also uh welcome back I I believe I believe used to work at at Google deepmind and now you've started your own company is that correct

(16:50) uh uh no actually uh I I used to work at Deep mine but now I'm I'm I'm work for a new company called quader uh so yeah I I industry actually and is that in the finance industry and quantitative quantitative research yeah quantitive research these days you can basically apply AI in any application and finance has been also very very interesting application very very cool so so tell me uh tell me about the uh models making new discoveries have you have you that that is the case or are they just next token predictors

(17:29) yeah yeah I mean that that's a like a philosophical and interesting question uh but uh I believe the answer to this question is is yes the models can definitely go beyond uh their training data and uh the technique that will uh let them do that is uh wellknown uh search basically so uh the first example uh that we we saw that was the uh work uh called fund search uh that he did in deep uh uh end of like 23 and uh in font search we basically used an llm to search in the space of uh Python programming language uh for let's say a

(18:07) very hard problem like an NP problem NP hard problem and uh the way it worked was that they basically paired an a large language model with a efficient uh let's say evaluator that would provide feedback on how good uh each solution is and then with an iterative algorithm it managed to discover new solutions for some open problems in Computer Science and Mathematics uh so the core idea was that the llm would basically propose uh let's say solu Solutions like a piece of code for a for a particular problem and

(18:38) then the evaluator would give some scores on on how good that solution is and then there was an evolutionary algorithm that would uh select the best solutions that the llm discovered so far it would present it to the llm and it would basically ask it to make it better so in some ways uh it was doing Best Shot prompting uh like a automatic way of prompting the llm we would just show it good Solutions we ask them to make better the LM comes up with something new we would show it again and and ask it to make it better and and this sort

(19:07) of process is repeated about 100 thousands of times and then the code that it starts with is something like a return zero and it would like evolve into something very interesting and complicated that would actually uh result in some new uh let's say solutions for open problems and it because this uh basically Solutions has never discovered by humans before it kind of shows that the llm discovered something new that wasn't in it training data and uh this concept has been like uh basically studied a lot like in the

(19:39) history of AI and recent examples uh recently it has become very popular in the modern llms and uh it's referred to by for example test time compute or inference scaling and uh other terminologies and and the way it works is that like uh you can basically uh uh when you ask a question for the llm uh you allow the M to think and the LM would basically generate multiple scenarios and generates like chains of reasonings and then it has a mechanism to understand that okay which one is more likely and then it would just

(20:10) iteratively repeat this process and by simulating different possibilities it basically search at inference time and with by searching at inference it it can manage to like basically discover something new because it can put a strap its own knowledge very cool um and and that makes me that makes me think that we've only just begun to tap into the kinds of things that that large language models will be able to accomplish or to achieve ex really really excited to to see what the next couple of years hold exactly

(20:43) like I I think like Rich sat puts it the best way we have like in the bitter lesson that like we have two techniques that basically scale infinitely uh the first one is like learning which sort of be cracked by like you know large amount of data and and just like the like the first let's say Generations of models we saw and then the next technique is search and I think we are at the moment where where people are are starting to scale search and uh throwing compute at inference time and that's going to
