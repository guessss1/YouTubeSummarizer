(21:10) unlock a whole lot of new capabilities especially for like reasoning capabilities and that's going to be very exciting excellent um thank you so much I I think I I think this also leads uh is a great lead to our next question um which was from our Discord app um uh uh and I think this is for Majid uh Majid you're our inference lead for the Gemini app uh yep excellent and uh so so this question is is really um is really all about uh can larger language models be used to train smaller ones um can we take some of this knowledge that that

(21:51) might have been obtained from a larger model and use it to improve smaller models um uh things that uh kind of like the model that we embed within many of the product surfaces at Google um what are your thoughts about that uh yeah absolutely uh yeah absolutely we can and yeah it's a very good question um this uh technique is called distillation uh it's a very popular inference optimization technique um and the main idea is as you said paig you you would take the knowledge from a very large model that might be too expensive to

(22:22) serve but has very good quality and then you would distill that knowledge into a smaller model um that can can retain a good amount of that knowledge and quality of the big model but is a at a size that you can serve um so yeah like I said it's a technique called distillation uh lots of active research and practice in in distillation um there are multiple types that you can distill Knowledge from a larger model to a smaller model um one of them is called Data distillation where you use the larger model to generate a lot of

(22:51) synthetic training data for the smaller model and then you just train the smaller model on that um another way we can do this is more fine grain where we try to align the token distribution for the smaller llm to be closer to the larger model and that's what we would call Knowledge distillation um and we talk about those types in the white paper uh that we're publishing with this with this class as well and the third type that I would mention is called on policy distillation where kind of going back to your discussion with Alexa on

(23:20) rhf we would be able to use the same reinforcement learning framework but in this case we would uh use the teacher model which is the larger model uh to effectively score output from the smaller model and use those scores to train and align the smaller model to to the larger model um so yeah distillation has multiple types they're all used in practice based on the use case and based on what we need um and actually I think in tomorrow's class one of those cases will be presented as well I think uh one

(23:49) of the authors for the gecko paper which uses llms to uh improve to produce much better embedding models using llms so you would use the llm to essentially produce query uh pass query uh queries for for passages and then you would train the embedding models on that and you would get a much much better performance I think it was something like 7x better for those embedding models of the same size using distillation um and I think jinuk Lee who's the author on that paper will be present in the vectors and embedding

(24:19) classes tomorrow very cool I I think uh I think what you just discussed will also be very helpful for the folks uh in our first pop quiz which should be coming up toward Wards the end of the hour um so I hope everybody was paying attention um all about the the great ways that you can uh use distillation to improve smaller models um and mechanisms uh mechanisms to improve uh to improve models at large um thank you so much um yep and next next question um I believe this is for you and Aunt um what are some approaches to evaluating large

(24:54) models you know we we've got uh we've got these models that have been created some larger versions some smaller Merion uh versions how do you pick which one that you should use for a given task good question Paige and yes um thanks for the community for asking this and just a heads up we'll be covering evaluation of large language models in more detail on day five we even have uh somebody from Deep Mind who'll be who' be presenting their research on it um and um yeah so but diving straight into

(25:24) question so there's uh evaluating large language models isn't an easy task because um it depends on the task you're evaluating and what you mean by evaluation there's of course classical natural language uh metrics like plow rou scores which you can use and this is one of the ways you can do it where you have a reference like a golden truth ground truth when you can see what the llm generated and then you can compare the the generated response with the human curated ground truth and you can see how to what extent it matches the
