11:02) text like you know you build chat apps you sort of process data you do a bunch of stuff it does feel like we're actually still pretty early in this like what are the real actual production use cases of multimodal output creation um both in image video and others like I I feel like we just like haven't uh there hasn't been as much ubiquitous access to those models and thus like developers haven't had enough time to really build interesting things um I think the the thing that I'm personally excited about

(11:32) is is actually like comparing or or rather bringing uh text to life using those things like I think there's just like a lot of text in the world and humans in a lot of cases are inherently visual um and I think if it's possible and we sort of saw this with notebook LM actually as an example like if you can take a bunch of written content and bring it to life in audio form it actually makes the experience like 10x better in some contexts and I can imagine you know the sort of uh I'm not on the notbook gan team so this isn't

(11:59) actually a product vision for them but uh you know you bring to life sort of the the huge Corpus of text documents you have in the form of a video instead of just like an audio conversation I think that's like a that's a crazy future world to to live in and I'm I I think it is going to be models like imagin in vo that actually um enable that mission to come to fruition so it's going to be um it's gonna be exciting and I see your lights just turned off this is folks who don't know the Google

(12:25) offices the lights turn off all the time so it's a it's a normal occurrence yes and the uh and I I love that use case as well about how you can uh sort of bring life to text documents or even to to things like GitHub code bases through video through audio through images um I think my favorite use case so far has been doing book trailers um so so taking in taking in a PDF book automatically you know segmenting it into into different into different prompts um and then creating a short video to describe the book I I I love it

(13:01) um and and would love to see love would see uh love to see more of that thing um so thank you so much this is this has been wonderful um I am going to pull up uh pull up our next question um which is all about reinforcement learning with human feedback um and I'm going to bring uh bring on stage Alexi Alexi you're uh correct me if I'm wrong you're our lead for rhf for the for the Gemini app is that correct correct excellent thank you and uh I'm really uh I'm really curious to learn more about how uh the Gemini

(13:40) app is using rhf to help improve its responses um uh how do how does that process work how do you uh sort of use user feedback um to to improve improve the models over time or to improve your app experiences over time yeah um so I mean it's a very big top uh so maybe it makes sense uh to say a few words uh on a high level how the llms are fine-tuned so like the def Factor recipe today for post trining uh state-of-the-art llms is U to do it in two steps sft and rlf sft is supervised fine tuning where we use demonstration

(14:21) data and I mean typically to get good results uh this data needs to be of extremely high quality and it's most often produced by human experts so it's quite expensive to get rhf is a a powerful technique uh for aligning models to human preferences um yeah think of like making responses safer more helpful or factually correct and uh it's a bit more complicated than sft um but in brief uh we use a collection of input prompts to generate responses from the model that we're trying to fine-tune and then un

(14:59) use another model called reward model to penalize responses uh that are bad and reward the good ones and this reward model can be uh fine-tuned on different types of preference data so typically it's human preference data this is what a relish stands for like human uh reinforcement learning with human feedback um and this data can be collected from Human expert raters just like um sft data or it can also come from um uh user feedback so people like users interacting with Gemini uh chatbot on on the desktop or their mobiles so

(15:38) the thumbs up thumbs down uh interactions that we that we uh do with the models actually actually lead to better performance over time is that what you're saying yeah it helps us to produce models that are perceived as more helpful uh to our users so this is kind of one of the primary mechanisms to align uh our models uh to human preferences amazing that's that's really exciting to hear and I know folks have been learning all about rhf and all about improving models with things like human feedback um through the through
